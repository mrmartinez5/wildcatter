{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7109b16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PPO Hyperparameter Tuning\n",
    "\n",
    "Here we will use Ray's Tune to search for the optimal hyperparameters of RLlib's Proximal Policy Optimization (PPO) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a9d588-7775-466c-9c07-ab404ea9b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildcatter.advanced_environment_for_RLib import AdvancedDriller\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from gym.spaces import Box, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "tf1, tf, tfv = try_import_tf(error=True)\n",
    "\n",
    "class WildcatterActionMaskedModel(TFModelV2):\n",
    "     \n",
    "    def __init__(self, \n",
    "                 obs_space,\n",
    "                 action_space,\n",
    "                 num_outputs,\n",
    "                 model_config,\n",
    "                 name,\n",
    "                 true_obs_shape=(11,40),\n",
    "                 action_embed_size=4+38+1,\n",
    "                 *args, **kwargs):\n",
    "         \n",
    "        super(WildcatterActionMaskedModel, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, \n",
    "            *args, **kwargs)\n",
    "         \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(-np.inf, np.inf, shape=true_obs_shape), \n",
    "                action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embed\")\n",
    " \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        # Compute the predicted action embedding\n",
    "        action_embed, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"obs\"]})\n",
    "        # Mask out invalid actions (use tf.float32.min for stability)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "        # Return action_logits + inf_mask, state\n",
    "        return action_embed + inf_mask, state\n",
    " \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    \n",
    "ModelCatalog.register_custom_model('wildcatter_masked', WildcatterActionMaskedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ca1722-3830-4bc3-a452-e411172276f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_random_config = dict(model_type = \"random\",\n",
    "                  nrow=11,\n",
    "                  ncol=40,\n",
    "                  funds=20,\n",
    "                  oil_price = 40,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                 )\n",
    "\n",
    "env_random_pockets_config = dict(model_type = \"random_pockets\",\n",
    "                  nrow=11,\n",
    "                  ncol=40,\n",
    "                  #nrow=40,\n",
    "                  #ncol=80,\n",
    "                  funds=20,\n",
    "                  oil_price = 1,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                 )\n",
    "\n",
    "env_2d_from_csv_config = dict(model_type = \"from_csv\",\n",
    "                  #model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/2d_two_rectangular_targets.csv\",\n",
    "                  #model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/2d_stacked.csv\",\n",
    "                  model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/x-sec_targets.csv\",\n",
    "                  delim=\",\",\n",
    "                  funds=20,\n",
    "                  oil_price = 40,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                  )\n",
    "\n",
    "env_config = env_random_pockets_config\n",
    "env = AdvancedDriller(env_config)\n",
    "# Setting variables for PPO trainer\n",
    "true_obs_shape = env.observation_space[\"obs\"].shape\n",
    "action_embed_size = env.action_space.n\n",
    "# Environment can be registered for easy access, if needed.\n",
    "#tune.register_env('wildcatter_driller', lambda environment_config_dict: AdvancedDriller(environment_config_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3e4f50-bf5b-48b0-8d08-cc8f965b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43e609c-3235-415c-a6a8-68aa35a9d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trial Plateau stopper that computes St.D. of windowed average\n",
    "from typing import Dict, Optional\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "from ray.tune import Stopper\n",
    "\n",
    "class MyTrialPlateauStopper(Stopper):\n",
    "    \"\"\"Early stop single trials when their windowed average reaches a plateau.\n",
    "\n",
    "    When the standard deviation of the window-averaged `metric` result of a trial is\n",
    "    below a threshold `std`, the trial plateaued and will be stopped\n",
    "    early.\n",
    "\n",
    "    Args:\n",
    "        metric: Metric to check for convergence.\n",
    "        std: Maximum metric standard deviation to decide if a\n",
    "            trial plateaued. Defaults to 0.01.\n",
    "        num_results: Number of results to consider for stdev\n",
    "            calculation.\n",
    "        grace_period: Minimum number of timesteps before a trial\n",
    "            can be early stopped\n",
    "        metric_threshold (Optional[float]):\n",
    "            Minimum or maximum value the result has to exceed before it can\n",
    "            be stopped early.\n",
    "        mode: If a `metric_threshold` argument has been\n",
    "            passed, this must be one of [min, max]. Specifies if we optimize\n",
    "            for a large metric (max) or a small metric (min). If max, the\n",
    "            `metric_threshold` has to be exceeded, if min the value has to\n",
    "            be lower than `metric_threshold` in order to early stop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric: str,\n",
    "        std: float = 15,\n",
    "        num_results: int = 10,\n",
    "        grace_period: int = 10,\n",
    "        metric_threshold: Optional[float] = None,\n",
    "        mode: Optional[str] = None,\n",
    "    ):\n",
    "        self._metric = metric\n",
    "        self._mode = mode\n",
    "\n",
    "        self._std = std\n",
    "        self._num_results = num_results\n",
    "        self._grace_period = grace_period\n",
    "        self._metric_threshold = metric_threshold\n",
    "\n",
    "        if self._metric_threshold:\n",
    "            if mode not in [\"min\", \"max\"]:\n",
    "                raise ValueError(\n",
    "                    f\"When specifying a `metric_threshold`, the `mode` \"\n",
    "                    f\"argument has to be one of [min, max]. \"\n",
    "                    f\"Got: {mode}\"\n",
    "                )\n",
    "\n",
    "        self._iter = defaultdict(lambda: 0)\n",
    "        self._trial_results = defaultdict(lambda: deque(maxlen=self._num_results))\n",
    "        self._trial_averages = defaultdict(lambda: deque(maxlen=self._num_results))\n",
    "\n",
    "    def __call__(self, trial_id: str, result: Dict):\n",
    "        metric_result = result.get(self._metric)\n",
    "        self._trial_results[trial_id].append(metric_result)\n",
    "        # Calculate running averages\n",
    "        try:\n",
    "            windowed_avg = np.average(self._trial_results[trial_id])\n",
    "        except Exception:\n",
    "            windowed_avg = metric_result\n",
    "        self._trial_averages[trial_id].append(windowed_avg)\n",
    "        self._iter[trial_id] += 1\n",
    "\n",
    "        # If still in grace period, do not stop yet\n",
    "        if self._iter[trial_id] < self._grace_period:\n",
    "            return False\n",
    "\n",
    "        # If not enough results yet, do not stop yet\n",
    "        if len(self._trial_results[trial_id]) < self._num_results:\n",
    "            return False\n",
    "\n",
    "        # If metric threshold value not reached, do not stop yet\n",
    "        if self._metric_threshold is not None:\n",
    "            if self._mode == \"min\" and metric_result > self._metric_threshold:\n",
    "                return False\n",
    "            elif self._mode == \"max\" and metric_result < self._metric_threshold:\n",
    "                return False\n",
    "\n",
    "        # Calculate stdev of last `num_results` averages\n",
    "        try:\n",
    "            current_std = np.std(self._trial_averages[trial_id])\n",
    "        except Exception:\n",
    "            current_std = float(\"inf\")\n",
    "\n",
    "        # If stdev is lower than threshold, stop early.\n",
    "        return current_std < self._std\n",
    "\n",
    "    def stop_all(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c554e-8afd-46bc-839d-ec0c7f7a6e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune.stopper import ExperimentPlateauStopper\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler as ASHAScheduler\n",
    "from ray.air.config import CheckpointConfig\n",
    "#from ray.tune import Callback\n",
    "\n",
    "param_space_config = ppo.DEFAULT_CONFIG.copy()\n",
    "special_config = {\"num_gpus\" : 0,\n",
    "                  \"num_workers\" : 0,\n",
    "                  \"num_envs_per_worker\" : 1,\n",
    "                  \"env\": AdvancedDriller,\n",
    "                  \"env_config\": env_config,\n",
    "                  \"model\": {\n",
    "                      \"custom_model\": \"wildcatter_masked\",\n",
    "                      \"custom_model_config\": {\n",
    "                          \"true_obs_shape\":true_obs_shape,\n",
    "                          \"action_embed_size\":action_embed_size,\n",
    "                      },\n",
    "                  },\n",
    "                  \"framework\": \"tf2\",\n",
    "                  \"horizon\" : 40,\n",
    "                  #\"clip_param\" : tune.uniform(0.1, 0.4),\n",
    "                  \"clip_param\" : 0.2,\n",
    "                  \"eager_tracing\" : True,\n",
    "                  #\"entropy_coeff\" : tune.loguniform(1e-8, 1e-1),\n",
    "                  \"entropy_coeff\" : 0,\n",
    "                  \"gamma\" : tune.loguniform(0.9, 0.9999),\n",
    "                  #\"gamma\" : 0.9,\n",
    "                  #\"lambda\" : tune.loguniform(0.8, 1.0),\n",
    "                  \"lambda\" : 1,\n",
    "                  \"lr\" : tune.loguniform(1e-5, 1e-2),\n",
    "                  \"sgd_minibatch_size\" : tune.choice([8, 16, 32, 64, 128, 256, 512]),\n",
    "                  #\"sgd_minibatch_size\" : 128,\n",
    "                  #\"vf_loss_coeff\" : tune.uniform(0,1),\n",
    "                  \"vf_loss_coeff\" : 1.0,\n",
    "                  \"num_sgd_iter\" : tune.choice([4, 8, 16, 32]),\n",
    "                  #\"num_sgd_iter\" : 30,\n",
    "                 }\n",
    "param_space_config.update(special_config)\n",
    "\n",
    "# Run config\n",
    "stopper = MyTrialPlateauStopper(\"episode_reward_mean\", std = 0.38, num_results = 10, grace_period = 10)\n",
    "\n",
    "myCheckpointConfig = CheckpointConfig(num_to_keep = 3,\n",
    "                                      checkpoint_frequency = 10,\n",
    "                                      checkpoint_at_end = True)\n",
    "\n",
    "myRunConfig = air.RunConfig(name = \"Test0003\",\n",
    "                            local_dir = \"~/sagemaker-studiolab-notebooks/\",\n",
    "                            stop = stopper,\n",
    "                            checkpoint_config = myCheckpointConfig,\n",
    "                            log_to_file = True,\n",
    "                            verbose = 1,\n",
    "                           )\n",
    "\n",
    "# Tune config\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='time_total_s',\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    max_t=30*60,\n",
    "    grace_period=2*60,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)\n",
    "\n",
    "myTuneConfig = tune.TuneConfig(scheduler=asha_scheduler,\n",
    "                               num_samples=1000,\n",
    "                               time_budget_s = 9.5 * 60 * 60, # In seconds. It exits gracefully when the wall clock time is reached.\n",
    "                              )\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "tuner = Tuner(\"PPO\", run_config=myRunConfig, param_space=param_space_config, tune_config=myTuneConfig )\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e947f02-6b97-4a9f-917e-3412cc30c202",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_annotated',\n",
       " '_experiment_analysis',\n",
       " '_populate_exception',\n",
       " '_trial_to_result',\n",
       " 'errors',\n",
       " 'get_best_result',\n",
       " 'get_dataframe',\n",
       " 'num_errors',\n",
       " 'num_terminated']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we need to continue a previously-interrupted tuning experiment\n",
    "tuner = Tuner.restore( path=\"~/ray_results/PPO\" )\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02afb76-a15e-4059-8df9-067b3e248886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'wildcatter.advanced_environment_for_RLib.AdvancedDriller'>, 'env_config': {'model_type': 'random_pockets', 'nrow': 11, 'ncol': 40, 'available_pipe': 30, 'available_wells': 3, 'oil_price': 1, 'relocation_cost': 0.2, 'drilling_cost': 0.5, 'drilling_depth_markup': 0.1}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 0, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': 40, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.918832998374206, 'lr': 0.0001389204072946106, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'wildcatter_masked', 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'wildcatter.advanced_environment_for_RLib.AdvancedDriller'>, 'env_config': {'model_type': 'random_pockets', 'nrow': 11, 'ncol': 40, 'available_pipe': 30, 'available_wells': 3, 'oil_price': 1, 'relocation_cost': 0.2, 'drilling_cost': 0.5, 'drilling_depth_markup': 0.1}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 0, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': 40, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.918832998374206, 'lr': 0.0001389204072946106, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'wildcatter_masked', 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7fc6cc1ebca0>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': True, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7fc6ec163c70>}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", results.get_best_result('episode_reward_mean', mode = 'max').config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf84d1d-e2da-4f87-bc80-635fce0f1a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal clip_param: 0.2\n",
      "Optimal entropy_coeff: 0.0\n",
      "Optimal gamma: 0.9976181731890753\n",
      "Optimal lambda: 1\n",
      "Optimal learning rate: 0.00071177770995787\n",
      "Optimal sgd_minibatch_size: 512\n",
      "Optimal vf_loss_coeff: 1.0\n",
      "Optimal num_sgd_iter: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal clip_param:\",results.get_best_result('episode_reward_mean', mode = 'max').config['clip_param'])\n",
    "print(\"Optimal entropy_coeff:\",results.get_best_result('episode_reward_mean', mode = 'max').config['entropy_coeff'])\n",
    "print(\"Optimal gamma:\",results.get_best_result('episode_reward_mean', mode = 'max').config['gamma'])\n",
    "print(\"Optimal lambda:\",results.get_best_result('episode_reward_mean', mode = 'max').config['lambda'])\n",
    "print(\"Optimal learning rate:\",results.get_best_result('episode_reward_mean', mode = 'max').config['lr'])\n",
    "print(\"Optimal sgd_minibatch_size:\",results.get_best_result('episode_reward_mean', mode = 'max').config['sgd_minibatch_size'])\n",
    "print(\"Optimal vf_loss_coeff:\",results.get_best_result('episode_reward_mean', mode = 'max').config['vf_loss_coeff'])\n",
    "print(\"Optimal num_sgd_iter:\",results.get_best_result('episode_reward_mean', mode = 'max').config['num_sgd_iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483169e-da95-4896-80cf-c431c58571d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildcatter-ThreeAmigos:Python",
   "language": "python",
   "name": "conda-env-wildcatter-ThreeAmigos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
