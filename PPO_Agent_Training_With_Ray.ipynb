{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7109b16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## PPO Agent Training with Action Masking\n",
    "\n",
    "Here we will import the Proximal Policy Optimization (PPO) algorithm from Ray.RLlib and train it with minimal parameters.\n",
    "\n",
    "Action masking allows the policy to start learning the best strategy from the get-go, without having to first learn the rules of the game.\n",
    "Enforcing action masking in Ray requires the definition of a custom model, which post-processes the logits coming out of the actual model (here, a Fully Connceted Neural Network) setting to -inf those of forbidden actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a9d588-7775-466c-9c07-ab404ea9b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildcatter.advanced_environment_for_RLib import AdvancedDriller\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from gym.spaces import Box, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "tf1, tf, tfv = try_import_tf(error=True)\n",
    "\n",
    "class WildcatterActionMaskedModel(TFModelV2):\n",
    "     \n",
    "    def __init__(self, \n",
    "                 obs_space,\n",
    "                 action_space,\n",
    "                 num_outputs,\n",
    "                 model_config,\n",
    "                 name,\n",
    "                 true_obs_shape=(11,40),\n",
    "                 action_embed_size=4+38+1,\n",
    "                 *args, **kwargs):\n",
    "         \n",
    "        super(WildcatterActionMaskedModel, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, \n",
    "            *args, **kwargs)\n",
    "         \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(-np.inf, np.inf, shape=true_obs_shape), \n",
    "                action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embed\")\n",
    " \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        # Compute the predicted action embedding\n",
    "        action_embed, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"obs\"]})\n",
    "        # Mask out invalid actions (use tf.float32.min for stability)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "        # Return action_logits + inf_mask, state\n",
    "        return action_embed + inf_mask, state\n",
    " \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    \n",
    "ModelCatalog.register_custom_model('wildcatter_masked', WildcatterActionMaskedModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77993a-b16f-4d8d-b3f9-419354821968",
   "metadata": {},
   "source": [
    "## Setting environment config dictionary\n",
    "\n",
    "Here we list the config dictionaries for various environment types, selecting the one we want to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ca1722-3830-4bc3-a452-e411172276f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_random_config = dict(model_type = \"random\",\n",
    "                  nrow=11,\n",
    "                  ncol=40,\n",
    "                  funds=20,\n",
    "                  oil_price = 1,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                 )\n",
    "\n",
    "env_random_pockets_config = dict(model_type = \"random_pockets\",\n",
    "                  nrow=11,\n",
    "                  ncol=40,\n",
    "                  #nrow=40,\n",
    "                  #ncol=80,\n",
    "                  funds=20,\n",
    "                  oil_price = 1,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                 )\n",
    "\n",
    "env_2d_from_csv_config = dict(model_type = \"from_csv\",\n",
    "                  #model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/2d_two_rectangular_targets.csv\",\n",
    "                  #model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/2d_stacked.csv\",\n",
    "                  model_path=r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/data/x-sec_targets.csv\",\n",
    "                  delim=\",\",\n",
    "                  funds=20,\n",
    "                  oil_price = 1,\n",
    "                  relocation_cost = 0.2,\n",
    "                  drilling_cost = 0.5,\n",
    "                  drilling_depth_markup = 0.1,\n",
    "                  #seed = 0,\n",
    "                  )\n",
    "\n",
    "env_config = env_random_pockets_config\n",
    "env = AdvancedDriller(env_config)\n",
    "# Setting variables for PPO trainer\n",
    "true_obs_shape = env.observation_space[\"obs\"].shape\n",
    "action_embed_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55a4c3-52f9-4e12-9925-b6e7bf7a8a72",
   "metadata": {},
   "source": [
    "## Loading the PPO trainer\n",
    "\n",
    "Let's instantiate a PPO trainer with three workers (to exploit the 4 CPUs available on SageMaker Studio Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbfe87c-5b83-4574-a988-6694292e04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 23:58:14,332\tWARNING services.py:1882 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 4294967296 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.65gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-09-22 23:58:15,514\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-09-22 23:58:16,419\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-09-22 23:58:16,420\tWARNING ppo.py:350 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1334.\n",
      "2022-09-22 23:58:16,420\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-09-22 23:58:16,423\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=428)\u001b[0m 2022-09-22 23:58:21,648\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-09-22 23:58:23,963\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "special_config = {\"num_gpus\" : 0,\n",
    "                  \"num_workers\" : 3, # Parallel training!\n",
    "                  \"env\": AdvancedDriller,\n",
    "                  \"env_config\": env_config,\n",
    "                  \"model\": {\n",
    "                      \"custom_model\": \"wildcatter_masked\",\n",
    "                      \"custom_model_config\": {\n",
    "                          \"true_obs_shape\":true_obs_shape,\n",
    "                          \"action_embed_size\":action_embed_size,\n",
    "                      }\n",
    "                  },\n",
    "                  \"horizon\" : 40\n",
    "                 }\n",
    "config.update(special_config)\n",
    "trainer = ppo.PPO(config=config)\n",
    "# In case we want to restore a checkpointed trainer\n",
    "#trainer.restore(my_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e767d04-c186-4c19-bb26-437bb6a4d95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that action masking works\n",
    "env = trainer.env_creator(env_config)\n",
    "obs = env.reset()\n",
    "obs['action_mask'][:-1] = 0 # Having set to zero the action mask of all other actions, the only remaining valid action is number 42\n",
    "# Let's ask the model to select an action for 10k times\n",
    "actions = np.array([trainer.compute_single_action(obs) for i in range(10000)])\n",
    "# Check that the action being selected has always been 42\n",
    "all(actions==42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfb51ca-ffa2-4505-b348-92a1a6b53e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=428)\u001b[0m /home/studio-lab-user/.conda/envs/wildcatter-ThreeAmigos/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=428)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; episode_reward_mean: -84.93577981651376\n",
      "Epoch: 10; episode_reward_mean: 0.5613237639553429\n",
      "Epoch: 20; episode_reward_mean: 8.912571428571427\n",
      "Epoch: 30; episode_reward_mean: 14.056223175965664\n",
      "Epoch: 40; episode_reward_mean: 21.181541218637992\n",
      "Epoch: 50; episode_reward_mean: 21.170833333333334\n",
      "Epoch: 60; episode_reward_mean: 28.291585127201564\n",
      "Epoch: 70; episode_reward_mean: 28.21468253968254\n",
      "Epoch: 80; episode_reward_mean: 29.847286821705424\n",
      "Epoch: 90; episode_reward_mean: 31.814534883720928\n",
      "Final checkpoint saved\n",
      "agent_timesteps_total: 400200\n",
      "counters:\n",
      "  num_agent_steps_sampled: 400200\n",
      "  num_agent_steps_trained: 400200\n",
      "  num_env_steps_sampled: 400200\n",
      "  num_env_steps_trained: 400200\n",
      "custom_metrics: {}\n",
      "date: 2022-09-23_00-18-11\n",
      "done: false\n",
      "episode_len_mean: 7.7159309021113245\n",
      "episode_media: {}\n",
      "episode_reward_max: 56.7\n",
      "episode_reward_mean: 30.949520153550857\n",
      "episode_reward_min: -122.1\n",
      "episodes_this_iter: 521\n",
      "episodes_total: 62731\n",
      "experiment_id: 2c0e69614154413e9e426aa023e1c405\n",
      "hostname: default\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5533465147018433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010097390972077847\n",
      "        model: {}\n",
      "        policy_loss: -0.03945758938789368\n",
      "        total_loss: 8.360873222351074\n",
      "        vf_explained_var: -0.47309887409210205\n",
      "        vf_loss: 8.393515586853027\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 400200\n",
      "  num_agent_steps_trained: 400200\n",
      "  num_env_steps_sampled: 400200\n",
      "  num_env_steps_trained: 400200\n",
      "iterations_since_restore: 100\n",
      "node_ip: 169.254.255.2\n",
      "num_agent_steps_sampled: 400200\n",
      "num_agent_steps_trained: 400200\n",
      "num_env_steps_sampled: 400200\n",
      "num_env_steps_sampled_this_iter: 4002\n",
      "num_env_steps_trained: 400200\n",
      "num_env_steps_trained_this_iter: 4002\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 3\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 4002\n",
      "perf:\n",
      "  cpu_util_percent: 76.47999999999999\n",
      "  ram_util_percent: 19.9\n",
      "pid: 96\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08970386887552517\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.16040774075916367\n",
      "  mean_inference_ms: 0.9673262660579531\n",
      "  mean_raw_obs_processing_ms: 0.5774132851737798\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 7.7159309021113245\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 56.7\n",
      "  episode_reward_mean: 30.949520153550857\n",
      "  episode_reward_min: -122.1\n",
      "  episodes_this_iter: 521\n",
      "  hist_stats:\n",
      "    episode_lengths: [5, 5, 10, 28, 8, 10, 5, 7, 6, 6, 5, 11, 11, 6, 23, 7, 11, 6,\n",
      "      16, 7, 7, 7, 5, 9, 6, 6, 7, 5, 6, 7, 8, 10, 7, 17, 9, 7, 11, 1, 6, 5, 5, 5,\n",
      "      6, 6, 7, 5, 7, 6, 5, 13, 6, 7, 5, 7, 6, 15, 9, 7, 6, 6, 6, 7, 11, 5, 5, 5, 5,\n",
      "      7, 7, 5, 8, 21, 5, 5, 5, 6, 5, 21, 19, 7, 6, 6, 6, 5, 6, 7, 9, 5, 5, 8, 6, 8,\n",
      "      5, 13, 1, 5, 9, 23, 10, 5, 7, 7, 7, 6, 6, 5, 7, 8, 9, 21, 15, 6, 7, 6, 12, 7,\n",
      "      7, 6, 7, 8, 7, 6, 7, 7, 7, 6, 5, 7, 7, 6, 1, 7, 5, 7, 12, 6, 10, 11, 7, 7, 5,\n",
      "      13, 5, 6, 1, 15, 7, 10, 6, 29, 5, 9, 5, 12, 7, 7, 5, 11, 9, 6, 6, 27, 6, 7,\n",
      "      6, 6, 8, 6, 22, 10, 5, 7, 7, 17, 5, 9, 27, 6, 9, 5, 5, 6, 9, 5, 6, 7, 24, 18,\n",
      "      9, 7, 5, 16, 5, 6, 10, 10, 6, 6, 7, 5, 6, 1, 11, 6, 6, 7, 6, 1, 7, 5, 13, 10,\n",
      "      5, 6, 5, 5, 6, 9, 5, 6, 12, 5, 5, 5, 6, 6, 5, 6, 9, 7, 6, 9, 7, 14, 6, 16, 11,\n",
      "      7, 5, 7, 5, 5, 9, 5, 9, 6, 6, 6, 5, 20, 7, 5, 7, 1, 6, 23, 5, 6, 3, 8, 7, 7,\n",
      "      1, 5, 6, 5, 9, 6, 6, 6, 6, 7, 12, 15, 23, 6, 6, 7, 12, 7, 5, 12, 12, 5, 1, 6,\n",
      "      1, 8, 14, 5, 6, 5, 9, 6, 7, 9, 3, 13, 12, 12, 7, 9, 6, 5, 6, 6, 10, 6, 6, 6,\n",
      "      7, 17, 15, 7, 6, 8, 12, 5, 11, 7, 8, 6, 7, 6, 7, 9, 6, 6, 5, 7, 5, 5, 6, 6,\n",
      "      6, 6, 7, 8, 26, 6, 6, 6, 8, 8, 6, 1, 7, 5, 5, 6, 12, 9, 9, 6, 7, 21, 9, 5, 7,\n",
      "      7, 6, 6, 23, 7, 13, 14, 6, 6, 6, 1, 6, 6, 6, 6, 6, 4, 7, 9, 6, 6, 6, 5, 6, 6,\n",
      "      7, 10, 7, 5, 6, 5, 12, 5, 5, 15, 6, 6, 6, 9, 9, 7, 5, 6, 8, 8, 6, 5, 6, 5, 6,\n",
      "      6, 29, 20, 7, 9, 5, 6, 5, 6, 5, 18, 5, 8, 6, 5, 9, 5, 7, 6, 5, 9, 12, 5, 6,\n",
      "      6, 7, 13, 27, 5, 7, 9, 7, 12, 6, 6, 6, 19, 6, 6, 11, 7, 6, 6, 6, 6, 5, 5, 5,\n",
      "      5, 9, 7, 7, 5, 6, 6, 6, 11, 5, 6, 7, 1, 5, 13, 6, 7, 7, 5, 6, 1, 6, 7, 1, 8,\n",
      "      5, 5, 6, 6, 7, 6, 8, 6, 7, 6, 5, 6, 7, 19, 7, 5, 5, 8, 5, 1, 6, 21, 6, 6, 7,\n",
      "      20, 5, 7, 7, 7, 7, 5, 6, 12, 1, 10, 6, 6, 5]\n",
      "    episode_reward: [-0.2, -2.2, 24.1, -112.4, 52.1, 10.0, 46.7, 17.9, 39.9, 17.9,\n",
      "      20.7, 17.5, 1.7000000000000002, 23.9, -121.7, -2.7, -9.200000000000001, 53.9,\n",
      "      -16.1, 43.0, 47.0, 53.0, 54.7, 25.1, 49.9, 53.9, 53.0, 42.7, 53.9, 37.0, 30.0,\n",
      "      33.9, 47.0, -102.7, 52.8, 21.299999999999997, -3.4, 0.0, 27.9, 54.7, 40.7, 44.7,\n",
      "      53.9, 37.9, 53.0, 30.7, 49.0, 36.0, 54.7, 16.2, 53.9, 31.0, 18.7, 21.1, 53.9,\n",
      "      -5.699999999999999, 52.8, 41.0, 37.9, 41.9, 53.9, 22.3, 15.0, 54.7, 26.7, 48.7,\n",
      "      40.7, 45.0, 43.0, 30.7, 19.7, -9.1, 54.7, 54.7, 54.7, 49.9, -4.4, 36.0, 13.8,\n",
      "      53.0, 49.9, 53.9, 33.9, 54.7, 53.9, 45.0, 18.4, 34.7, 54.7, 50.3, 53.9, 46.2,\n",
      "      54.7, -10.200000000000001, 0.0, 28.7, 24.9, 20.099999999999998, 46.3, 30.7,\n",
      "      41.0, 41.0, 53.0, 55.9, 29.9, 26.7, 41.0, 6.4, 52.8, 5.300000000000001, -6.699999999999999,\n",
      "      41.9, 47.0, 45.9, -3.9, 34.099999999999994, 53.0, 37.9, 43.0, 17.3, 13.0, 41.9,\n",
      "      53.2, 53.0, 21.0, 49.9, 54.7, 25.0, 41.0, 35.9, 0.0, 27.2, 54.7, 39.0, 50.3,\n",
      "      43.9, 28.0, 52.2, -2.9999999999999996, 29.0, 18.7, 43.8, 38.7, 17.9, 0.0, 25.6,\n",
      "      53.0, 51.8, 41.9, -122.0, 54.7, 26.7, 44.7, -7.8999999999999995, 53.0, 53.0,\n",
      "      54.7, 35.5, 14.8, 15.9, 39.9, -120.1, 21.9, 53.0, -2.0999999999999996, 35.9,\n",
      "      54.5, 53.9, -120.0, 46.2, 56.7, 53.0, 53.0, 6.199999999999999, 46.7, 33.0, -120.0,\n",
      "      -2.0999999999999996, 38.8, 44.7, 18.7, 35.9, 52.8, 46.7, 31.9, 47.1, 6.499999999999999,\n",
      "      -12.299999999999997, 52.8, 49.0, 54.7, -13.399999999999999, 54.7, 25.9, 46.3,\n",
      "      27.6, 49.9, 29.9, 53.0, 54.7, 53.9, 0.0, 19.2, 41.9, 21.9, 41.3, 53.9, 0.0,\n",
      "      53.0, 36.7, -9.7, -3.3, 54.7, 53.9, 24.7, 18.7, 47.9, 38.0, 30.7, 53.9, 19.7,\n",
      "      18.7, 54.7, 50.7, 53.9, -2.0999999999999996, 36.7, 53.9, 39.4, 41.0, 31.9, -3.1999999999999997,\n",
      "      53.0, 36.3, 53.9, 39.1, 43.1, 53.0, 54.7, 47.0, 50.7, -1.2999999999999998, -2.3000000000000003,\n",
      "      22.7, 52.8, 33.9, 53.9, 53.9, 44.7, -9.899999999999999, 21.0, 54.7, 35.0, 0.0,\n",
      "      35.9, -122.1, 54.7, 33.9, 0.0, 50.3, 33.0, -4.9, 0.0, 46.7, 53.9, 54.7, 49.2,\n",
      "      53.9, 53.9, 53.9, 53.9, 53.0, -4.3, 13.4, -15.999999999999998, 49.9, 17.9, 53.0,\n",
      "      50.3, 27.0, 54.7, 44.3, 50.1, 54.7, 0.0, 41.9, 0.0, 52.1, 8.799999999999999,\n",
      "      34.7, 23.9, 40.7, 38.8, 53.9, 41.0, 40.8, 0.0, 32.9, -8.6, 46.3, 53.0, -3.1999999999999997,\n",
      "      39.9, 34.7, 41.9, 33.9, -4.199999999999999, 53.9, 53.9, 31.9, 53.0, -10.899999999999999,\n",
      "      -10.3, 53.0, 41.9, 20.5, 8.9, 22.7, 18.200000000000003, 27.0, -4.0, 53.9, 45.0,\n",
      "      -2.0999999999999996, 16.5, 14.9, 45.9, 53.9, 42.7, 31.0, 48.7, 28.7, 53.9, 41.9,\n",
      "      23.9, 33.9, 41.0, 33.9, -17.099999999999994, 41.9, 53.9, 53.9, 2.2000000000000006,\n",
      "      40.0, 53.9, 0.0, 35.0, 54.7, 36.7, 53.9, 50.3, 21.1, 52.8, 53.9, 47.0, -18.1,\n",
      "      33.5, 36.7, 21.0, 53.0, 53.9, 55.9, -17.5, -2.8999999999999995, -6.2, -3.3999999999999986,\n",
      "      35.9, 13.9, 39.9, 0.0, 53.9, 53.9, 39.9, 27.9, 41.9, 43.4, 53.0, 48.8, 53.9,\n",
      "      47.9, 53.9, -1.2999999999999998, 41.9, 21.9, 35.0, 15.6, 33.7, 36.7, 53.9, 54.7,\n",
      "      -8.200000000000001, 54.7, 42.7, 12.1, 53.9, 27.9, 29.9, 52.8, 40.8, 41.0, 54.7,\n",
      "      33.9, -2.8999999999999995, 52.1, 43.9, 46.7, 43.9, 34.7, 29.9, 53.9, -120.5,\n",
      "      -8.4, 53.0, 15.200000000000001, 18.7, 17.9, 38.7, 13.9, 54.7, 3.0999999999999988,\n",
      "      42.7, 47.9, 19.9, 54.7, 24.3, 32.7, 14.899999999999999, 27.9, 34.7, 39.1, -3.6999999999999997,\n",
      "      54.7, 53.9, 19.9, 53.0, 37.3, 5.3999999999999995, 54.7, -2.9999999999999996,\n",
      "      48.8, 35.0, -9.3, 27.9, 53.9, 53.9, -13.500000000000002, 49.9, 53.9, 39.5, -1.6,\n",
      "      41.9, 39.9, 25.9, 39.9, 42.7, -1.2999999999999998, 54.7, 34.7, -3.1999999999999997,\n",
      "      31.0, 53.7, 52.7, -2.0999999999999996, 21.9, 43.9, 23.8, 18.7, 21.9, 21.0, 0.0,\n",
      "      40.7, 12.3, 53.9, 53.0, 34.3, 48.7, 53.9, 0.0, 31.9, 53.0, 0.0, 39.7, 54.7,\n",
      "      54.7, 41.9, 41.9, 39.0, 49.9, 52.1, 29.9, 53.0, 45.9, 22.7, 3.9000000000000004,\n",
      "      37.0, -10.6, 31.0, 40.7, 38.7, 52.1, 54.7, 0.0, 33.9, 39.1, 29.9, 53.9, 35.3,\n",
      "      20.5, 40.7, 53.0, 41.099999999999994, 45.0, 53.0, 38.7, 49.9, 48.3, 0.0, -5.2,\n",
      "      35.9, 45.9, 38.7]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08970386887552517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16040774075916367\n",
      "    mean_inference_ms: 0.9673262660579531\n",
      "    mean_raw_obs_processing_ms: 0.5774132851737798\n",
      "time_since_restore: 676.141681432724\n",
      "time_this_iter_s: 6.86707615852356\n",
      "time_total_s: 676.141681432724\n",
      "timers:\n",
      "  learn_throughput: 943.214\n",
      "  learn_time_ms: 4242.938\n",
      "  load_throughput: 17685812.462\n",
      "  load_time_ms: 0.226\n",
      "  synch_weights_time_ms: 3.327\n",
      "  training_iteration_time_ms: 6663.641\n",
      "timestamp: 1663892291\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400200\n",
      "training_iteration: 100\n",
      "trial_id: default\n",
      "warmup_time: 516.5494296550751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's do 1000 training epochs\n",
    "for i in range(100):\n",
    "    result = trainer.train()\n",
    "    if i%10 == 0:\n",
    "    #    checkpoint = trainer.save(\"my_checkpoint\")\n",
    "        print(f\"Epoch: {i}; episode_reward_mean: {result.get('episode_reward_mean')}\")\n",
    "\n",
    "checkpoint = trainer.save(\"my_checkpoint\")\n",
    "print(\"Final checkpoint saved\")\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecdbe5-92c3-4a32-a4e6-f0586e90713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check policy\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "eval_config = ppo.DEFAULT_CONFIG.copy()\n",
    "special_config = {\"num_workers\" : 0,\n",
    "                  \"env\": AdvancedDriller,\n",
    "                  \"env_config\": env_config,\n",
    "                  \"model\": {\n",
    "                      \"custom_model\": \"wildcatter_masked\",\n",
    "                      \"custom_model_config\": {\n",
    "                          \"true_obs_shape\":true_obs_shape,\n",
    "                          \"action_embed_size\":action_embed_size,\n",
    "                      },\n",
    "                  },\n",
    "                  \"horizon\" : 40,\n",
    "                  \"explore\" : False, # Always returns best action\n",
    "                 }\n",
    "eval_config.update(special_config)\n",
    "agent = ppo.PPO(config=eval_config, env=AdvancedDriller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c826e7-4af4-42b5-8bed-ea3bcafb0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we restore an agent we trained for 400 epochs with the standard parameters\n",
    "agent.restore(r\"/home/studio-lab-user/sagemaker-studiolab-notebooks/wildcatter-ThreeAmigos/examples/my_checkpoint/checkpoint_000400/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ad42d-f7ce-4ce0-a85f-685e6a751f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_config = env_random_pockets_config\n",
    "#env_config = env_2d_two_rectangular_targets_config\n",
    "env = AdvancedDriller(env_config)\n",
    "print(\"Beginning Drill Campaign\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.state, vmin=-10, vmax=2)\n",
    "plt.xticks(np.arange(0, env.ncol, 1.0))\n",
    "plt.yticks(np.arange(0, env.nrow, 1.0))\n",
    "plt.xlim([-0.5, env.ncol - 0.5])\n",
    "plt.ylim([env.nrow - 0.5, -0.5])\n",
    "plt.grid()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    print(f\"Action: {action}; funds: {obs['obs'][-1]}; reward: {reward}; total score: {score}\")\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.state, vmin=-10, vmax=2)\n",
    "for well in env.trajectory:\n",
    "    traj_z, traj_x = np.asarray(well).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", c=\"m\", linewidth=6)\n",
    "plt.xticks(np.arange(0, env.ncol, 1.0))\n",
    "plt.yticks(np.arange(0, env.nrow, 1.0))\n",
    "plt.xlim([-0.5, env.ncol - 0.5])\n",
    "plt.ylim([env.nrow - 0.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc5486-fd2e-4e09-b912-2b9ab864f292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildcatter-ThreeAmigos:Python",
   "language": "python",
   "name": "conda-env-wildcatter-ThreeAmigos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
